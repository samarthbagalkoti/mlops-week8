name: CI - Airflow DAGs & ML

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]

jobs:
  build-test:
    runs-on: ubuntu-latest
    env:
      PYTHON_VERSION: "3.11"
      AIRFLOW_VERSION: "2.9.2"
      AIRFLOW_HOME: ${{ github.workspace }}/.airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
      AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/dags
      # make local imports (src/) resolvable and plots headless
      PYTHONPATH: ${{ github.workspace }}
      MPLBACKEND: "Agg"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Upgrade pip and install base test/lint deps
        run: |
          python -m pip install --upgrade pip
          pip install pytest ruff==0.6.8

      - name: Install project deps (optional)
        run: |
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Install Airflow (pinned with constraints)
        run: |
          CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
          pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"

      # ---- Lint & Format (writes first, then verifies) ----
      - name: Ruff - format & lint (write)
        run: |
          ruff format dags src tests
          ruff check dags src tests --fix

      - name: Ruff - verify clean
        run: |
          ruff format dags src tests --check
          ruff check dags src tests

      # ---- Unit tests (host-level) ----
      - name: Unit tests (pytest)
        run: pytest -q

      # ---- Airflow checks ----
      - name: Init Airflow DB (SQLite)
        run: |
          mkdir -p "${AIRFLOW_HOME}"
          airflow db init
          airflow users create \
            --username admin --password admin --firstname Admin --lastname User \
            --role Admin --email admin@example.com || true

      - name: DAG import check (DagBag)
        run: |
          python - <<'PY'
          from airflow.models import DagBag
          db = DagBag(include_examples=False)
          assert db.import_errors == {}, f"Import errors: {db.import_errors}"
          print(f"Loaded {len(db.dags)} DAG(s) OK")
          PY

      - name: List DAGs (sanity)
        run: airflow dags list -S "${{ github.workspace }}/dags"

      # Run the entire producer DAG once for a specific execution date
      - name: Airflow "dags test" - dataset_producer_dag (full DAG)
        run: airflow dags test -S "${{ github.workspace }}/dags" dataset_producer_dag 2025-09-01

      # Run a single task from producer DAG (use your actual task_id; default below is write_dataset)
      - name: Airflow "tasks test" - dataset_producer_dag.write_dataset
        run: airflow tasks test -S "${{ github.workspace }}/dags" dataset_producer_dag write_dataset 2025-09-01

      # Run the param DAG end-to-end with a small conf
      - name: Airflow "dags test" - mlops_w8_param_pipeline with conf
        run: >
          airflow dags test -S "${{ github.workspace }}/dags"
          mlops_w8_param_pipeline 2025-09-01
          -c '{"C": 1.0, "max_iter": 10}'
